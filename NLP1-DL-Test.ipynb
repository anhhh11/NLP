{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "torch.manual_seed(1)\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "Variable = torch.autograd.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self, stop_words, counter):\n",
    "        self.stop_words = stop_words\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.stop_words = {}\n",
    "        self.counter = counter\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx and word not in self.stop_words:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    def get_text_indexes(self, text):\n",
    "         return [(self(token),self.counter[token]) for token in nltk.tokenize.word_tokenize(text.lower()) if token not in self.stop_words]\n",
    "    \n",
    "    def get_text_string(self, indexes):\n",
    "        return [self.idx2word[index] for index in indexes]\n",
    "    \n",
    "\n",
    "def build_vocab(texts, stop_words, threshold = 2):\n",
    "    counter = Counter()\n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = [t for t in nltk.tokenize.word_tokenize(text.lower()) if t not in stop_words and len(t) > 1]\n",
    "        counter.update(tokens)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"[%d/%d] Tokenized the texts.\" %(i, len(texts)))\n",
    "    \n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "    \n",
    "    vocab = Vocabulary(stop_words, counter)\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "    \n",
    "class Model(torch.nn.Module) :\n",
    "    def __init__(self,embedding_dim,hidden_dim,vocab_limit,number_of_class) :\n",
    "        super(Model,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_limit, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim + 1,hidden_dim,dropout=0.5)\n",
    "        self.linear1 = nn.Linear(hidden_dim,number_of_class)\n",
    "    def forward(self,inputs,bow,hidden) :\n",
    "        x = self.embeddings(inputs).view(len(inputs),1,-1)\n",
    "        x = torch.cat([x,bow.view(x.size(0),x.size(1),1)], dim=2)\n",
    "        lstm_out1,lstm_h1 = self.lstm1(x,hidden)\n",
    "        x = lstm_out1[-1]\n",
    "        x = self.linear1(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x,lstm_h1\n",
    "    def init_hidden(self) :\n",
    "        return (Variable(torch.zeros(1, 1, self.hidden_dim).type(FloatTensor)),Variable(torch.zeros(1, 1, self.hidden_dim).type(FloatTensor)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "with open('./data/vietnamese-stopwords.txt', 'r') as file_stop_words:\n",
    "    for line in file_stop_words:\n",
    "        stop_words.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/vocabs.pkl','rb') as f:\n",
    "    vocabs = pickle.load(f)\n",
    "vocabs.stop_words = stop_words\n",
    "vocab_limit = len(vocabs)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.classes_ = np.load('./data/classes.npy')\n",
    "number_of_class = le.classes_.size\n",
    "\n",
    "model = Model(80,100, vocab_limit, number_of_class)\n",
    "model.load_state_dict(torch.load('./data/model_last.pth'))\n",
    "if(use_cuda):\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./data/DevTesting.txt', 'r')\n",
    "data = []\n",
    "X = [] # text \n",
    "y = [] # label (text)\n",
    "for line in file:\n",
    "    row = line.split(' ', 1)\n",
    "    data.append((row[1].strip(), row[0].split('__')[1]))\n",
    "    X.append(row[1].strip())\n",
    "    y.append(row[0].split('__')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/computer/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:65: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "X_sample = X\n",
    "Y_real = le.transform(y)\n",
    "Y_pred = []\n",
    "total_freq_words = sum(vocabs.counter.values())\n",
    "model.eval() # to disable drop out\n",
    "for idx, x in enumerate(X_sample):\n",
    "    input_data, input_data_bow = zip(*vocabs.get_text_indexes(x))\n",
    "    input_data = Variable(LongTensor(input_data))\n",
    "    # input_data_bow = F.normalize(FloatTensor(input_data_bow),dim=0)\n",
    "    input_data_bow = FloatTensor(input_data_bow) / total_freq_words\n",
    "    input_data_bow = Variable(input_data_bow)\n",
    "    hidden = model.init_hidden()\n",
    "    y_pred,_ = model(input_data,input_data_bow,hidden)\n",
    "    Y_pred.append(int(y_pred.max(1)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832\n"
     ]
    }
   ],
   "source": [
    "precise = np.count_nonzero(Y_real == Y_pred) / Y_real.size\n",
    "print(precise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    input_data, input_data_bow = zip(*vocabs.get_text_indexes(x))\n",
    "    input_data = Variable(LongTensor(input_data))\n",
    "    input_data_bow = F.normalize(FloatTensor(input_data_bow),dim=0)\n",
    "    input_data_bow = Variable(input_data_bow)\n",
    "    hidden = model.init_hidden()\n",
    "    y_pred,_ = model(input_data,input_data_bow,hidden)\n",
    "    return le.inverse_transform(int(y_pred.max(1)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/computer/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:65: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/computer/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CTXH'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Kiểm tra đột xuất, cơ quan chức năng phát hiện cơ sở thẩm mỹ không phép trong chung cư đang chuẩn bị “dao kéo” trên cơ thể người bệnh. Tại đây, nhiều loại thuốc không rõ nguồn gốc xuất xứ đã bị niêm phong chờ xử lý.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
