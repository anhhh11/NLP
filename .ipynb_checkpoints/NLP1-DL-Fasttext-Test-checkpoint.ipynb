{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.utils import tokenize\n",
    "torch.manual_seed(1)\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = FastText.load('/mnt/A20CC3B20CC37FB1/cc.vi.300.gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "Variable = torch.autograd.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.classes_ = np.load('./data/classes.npy')\n",
    "number_of_class = le.classes_.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pretrained(embeddings, freeze=True):\n",
    "    assert embeddings.dim() == 2, \\\n",
    "         'Embeddings parameter is expected to be 2-dimensional'\n",
    "    rows, cols = embeddings.shape\n",
    "    embedding = torch.nn.Embedding(num_embeddings=rows, embedding_dim=cols)\n",
    "    embedding.weight = torch.nn.Parameter(embeddings)\n",
    "    embedding.weight.requires_grad = not freeze\n",
    "    return embedding\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,weights,hidden_dim,number_of_class) :\n",
    "        super(Model,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        embedding_dim = weights.shape[1]\n",
    "        self.embeddings = from_pretrained(weights)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim,hidden_dim,dropout=0.2)\n",
    "        self.linear1 = nn.Linear(hidden_dim,number_of_class)\n",
    "    def forward(self,inputs,hidden) :\n",
    "        x = self.embeddings(inputs).view(len(inputs),1,-1)\n",
    "        lstm_out1,lstm_h1 = self.lstm1(x,hidden)\n",
    "        x = lstm_out1[-1]\n",
    "        x = self.linear1(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x,lstm_h1\n",
    "    def init_hidden(self) :\n",
    "        return (Variable(torch.zeros(1, 1, self.hidden_dim).type(FloatTensor)),Variable(torch.zeros(1, 1, self.hidden_dim).type(FloatTensor)))\n",
    "def get_indexes(text):\n",
    "    return [fasttext_model.wv.vocab.get(t).index for t in tokenize(text) if t in fasttext_model.wv.vocab]\n",
    "def save_params(model, i):\n",
    "    pl = list(model.parameters())\n",
    "    pl = [p for i,p in enumerate(pl) if i > 0]\n",
    "    torch.save(pl,'./data/params-%d.dat' % i)\n",
    "def load_params(model, params_path):\n",
    "    pl = list(model.parameters())\n",
    "    pl = [p for i,p in enumerate(pl) if i > 0]\n",
    "    pll = torch.load(params_path)\n",
    "    for p1, p2 in zip(pl, pll):\n",
    "        p1.data = p2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FloatTensor(fasttext_model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(weights,80,number_of_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_params(model, './data/params-29.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/computer/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/computer/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TG'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "\n",
    "file = open('./data/DevTesting.txt', 'r')\n",
    "data = []\n",
    "X = [] # text \n",
    "y = [] # label (text)\n",
    "for line in file:\n",
    "    row = line.split(' ', 1)\n",
    "    data.append((row[1].strip(), row[0].split('__')[1]))\n",
    "    X.append(row[1].strip())\n",
    "    y.append(row[0].split('__')[1])\n",
    "\n",
    "X_sample = X\n",
    "Y_real = le.transform(y)\n",
    "Y_pred = []\n",
    "model.eval() # to disable drop out\n",
    "for idx, x in enumerate(X_sample):\n",
    "    input_data = get_indexes(x)\n",
    "    input_data = Variable(LongTensor(input_data))\n",
    "    hidden = model.init_hidden()\n",
    "    y_pred,_ = model(input_data,hidden)\n",
    "    Y_pred.append(int(y_pred.max(1)[1]))\n",
    "\n",
    "Y_pred = np.array(Y_pred)\n",
    "\n",
    "precise = np.count_nonzero(Y_real == Y_pred) / Y_real.size\n",
    "print(precise)\n",
    "\n",
    "def predict(text):\n",
    "    input_data = get_indexes(text)\n",
    "    input_data = Variable(LongTensor(input_data))\n",
    "    hidden = model.init_hidden()\n",
    "    y_pred,_ = model(input_data,hidden)\n",
    "    return le.inverse_transform(int(y_pred.max(1)[1]))\n",
    "\n",
    "predict(\"Toàn cảnh hiện diện quân sự của Mỹ ở Syria\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
