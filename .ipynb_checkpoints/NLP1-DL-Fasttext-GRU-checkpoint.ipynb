{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = FastText.load('/mnt/A20CC3B20CC37FB1/cc.vi.300.gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/computer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "torch.manual_seed(10)\n",
    "# Enable inline plotting\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "Variable = torch.autograd.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./data/Training_shuf.txt', 'r')\n",
    "data = []\n",
    "X = [] # text \n",
    "y = [] # label (text)\n",
    "for line in file:\n",
    "    row = line.split(' ', 1)\n",
    "    data.append((row[1].strip(), row[0].split('__')[1]))\n",
    "    X.append(row[1].strip())\n",
    "    y.append(row[0].split('__')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "Y_train = le.transform(y)\n",
    "number_of_class = le.classes_.size\n",
    "np.save('./data/classes.npy',le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pretrained(embeddings, freeze=True):\n",
    "    assert embeddings.dim() == 2, \\\n",
    "         'Embeddings parameter is expected to be 2-dimensional'\n",
    "    rows, cols = embeddings.shape\n",
    "    embedding = torch.nn.Embedding(num_embeddings=rows, embedding_dim=cols)\n",
    "    embedding.weight = torch.nn.Parameter(embeddings)\n",
    "    embedding.weight.requires_grad = not freeze\n",
    "    return embedding\n",
    "class Model(torch.nn.Module) :\n",
    "    def __init__(self,weights,hidden_dim,number_of_class) :\n",
    "        super(Model,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        embedding_dim = weights.shape[1]\n",
    "        self.embeddings = from_pretrained(weights)\n",
    "        self.lstm1 = nn.GRU(embedding_dim,hidden_dim,dropout=0.2)\n",
    "        self.linear1 = nn.Linear(hidden_dim,number_of_class)\n",
    "    def forward(self,inputs,hidden) :\n",
    "        x = self.embeddings(inputs).view(len(inputs),1,-1)\n",
    "        lstm_out1,lstm_h1 = self.lstm1(x,hidden)\n",
    "        x = lstm_out1[-1]\n",
    "#         x = lstm_out1[:,-1,:].squeeze()\n",
    "        x = self.linear1(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x,lstm_h1\n",
    "    def init_hidden(self) :\n",
    "        return Variable(torch.randn(1, 1, self.hidden_dim),requires_grad=True).type(FloatTensor)\n",
    "        # return (Variable(torch.zeros(1, 1, self.hidden_dim).type(FloatTensor)),Variable(torch.zeros(1, 1, self.hidden_dim).type(FloatTensor)))\n",
    "def get_indexes(text):\n",
    "    return [fasttext_model.wv.vocab.get(t).index for t in tokenize(text) if t in fasttext_model.wv.vocab]\n",
    "def save_params(model, i):\n",
    "    pl = list(model.parameters())\n",
    "    pl = [p for i,p in enumerate(pl) if i > 0]\n",
    "    torch.save(pl,'./data/params-%d.dat' % i)\n",
    "def load_params(model, params_path):\n",
    "    pl = list(model.parameters())\n",
    "    pl = [p for i,p in enumerate(pl) if i > 0]\n",
    "    pll = torch.load(params_path)\n",
    "    for p1, p2 in zip(pl, pll):\n",
    "        p1.data = p2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FloatTensor(fasttext_model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(weights,80,number_of_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_cuda):\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_params(model, './data/params--1.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/computer/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/computer/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :0 iterations :1 loss :0.0430298\n",
      "epoch :0 iterations :500 loss :0.205671\n",
      "epoch :0 iterations :1000 loss :0.00860977\n",
      "epoch :0 iterations :1500 loss :0.0210786\n",
      "epoch :0 iterations :2000 loss :0.000677109\n",
      "epoch :0 iterations :2500 loss :0.0103216\n",
      "epoch :0 iterations :3000 loss :0.000962257\n",
      "epoch :0 iterations :3500 loss :0.00868368\n",
      "epoch :0 iterations :4000 loss :0.00387669\n",
      "epoch :0 iterations :4500 loss :0.00805092\n",
      "epoch :0 iterations :5000 loss :0.143642\n",
      "epoch :0 iterations :5500 loss :0.0169215\n",
      "epoch :0 iterations :6000 loss :0.00094223\n",
      "epoch :0 iterations :6500 loss :0.000814438\n",
      "epoch :0 iterations :7000 loss :0.00501537\n",
      "epoch :0 iterations :7500 loss :0.0525031\n",
      "epoch :0 iterations :8000 loss :0.00546122\n",
      "epoch :0 iterations :8500 loss :0.893671\n",
      "epoch :0 iterations :9000 loss :0.0123391\n",
      "epoch :0 iterations :9500 loss :0.00272942\n",
      "the average loss after completion of 1 epochs is 0.292063\n",
      "epoch :1 iterations :1 loss :0.0676866\n",
      "epoch :1 iterations :500 loss :0.0246425\n",
      "epoch :1 iterations :1000 loss :0.00570679\n",
      "epoch :1 iterations :1500 loss :0.00727129\n",
      "epoch :1 iterations :2000 loss :0.00033474\n",
      "epoch :1 iterations :2500 loss :0.00460339\n",
      "epoch :1 iterations :3000 loss :0.000139236\n",
      "epoch :1 iterations :3500 loss :0.00418043\n",
      "epoch :1 iterations :4000 loss :0.00283813\n",
      "epoch :1 iterations :4500 loss :0.00236702\n",
      "epoch :1 iterations :5000 loss :0.10341\n",
      "epoch :1 iterations :5500 loss :0.0119605\n",
      "epoch :1 iterations :6000 loss :0.000355721\n",
      "epoch :1 iterations :6500 loss :0.00091362\n",
      "epoch :1 iterations :7000 loss :0.00257874\n",
      "epoch :1 iterations :7500 loss :0.0367951\n",
      "epoch :1 iterations :8000 loss :0.00110722\n",
      "epoch :1 iterations :8500 loss :0.103518\n",
      "epoch :1 iterations :9000 loss :0.0174532\n",
      "epoch :1 iterations :9500 loss :0.00140572\n",
      "the average loss after completion of 2 epochs is 0.223739\n",
      "epoch :2 iterations :1 loss :0.0083847\n",
      "epoch :2 iterations :500 loss :0.0165024\n",
      "epoch :2 iterations :1000 loss :0.000809669\n",
      "epoch :2 iterations :1500 loss :0.00274992\n",
      "epoch :2 iterations :2000 loss :4.00543e-05\n",
      "epoch :2 iterations :2500 loss :0.000276566\n",
      "epoch :2 iterations :3000 loss :2.86102e-05\n",
      "epoch :2 iterations :3500 loss :0.000385284\n",
      "epoch :2 iterations :4000 loss :0.000537872\n",
      "epoch :2 iterations :4500 loss :0.00119305\n",
      "epoch :2 iterations :5000 loss :0.102115\n",
      "epoch :2 iterations :5500 loss :0.0102677\n",
      "epoch :2 iterations :6000 loss :8.67844e-05\n",
      "epoch :2 iterations :6500 loss :0.0011301\n",
      "epoch :2 iterations :7000 loss :0.00159073\n",
      "epoch :2 iterations :7500 loss :0.057632\n",
      "epoch :2 iterations :8000 loss :0.00138855\n",
      "epoch :2 iterations :8500 loss :0.316583\n",
      "epoch :2 iterations :9000 loss :0.00199604\n",
      "epoch :2 iterations :9500 loss :0.00313091\n",
      "the average loss after completion of 3 epochs is 0.174305\n",
      "CPU times: user 13min 12s, sys: 2min 40s, total: 15min 52s\n",
      "Wall time: 15min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 3\n",
    "\n",
    "print('starting training')\n",
    "X_sample = X#[2000:4000]\n",
    "Y_train_sample = Y_train#[2000:4000]\n",
    "for i in range(epochs):\n",
    "    avg_loss = 0.0\n",
    "    for idx, (x, y_train) in enumerate(zip(X_sample, Y_train_sample)):\n",
    "        if not idx == 0:\n",
    "            input_data = get_indexes(x)\n",
    "            input_data = Variable(LongTensor(input_data))\n",
    "            y_train = y_train.item()\n",
    "            target_data = Variable(LongTensor([y_train]))\n",
    "            hidden = model.init_hidden()\n",
    "            model.zero_grad()\n",
    "            y_pred,_ = model(input_data,hidden)\n",
    "            loss = loss_function(y_pred,target_data)\n",
    "            avg_loss += loss.data[0]\n",
    "            \n",
    "            if idx%500 == 0 or idx == 1:\n",
    "                print('epoch :%d iterations :%d loss :%g'%(i,idx,loss.data[0]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    save_params(model, i)\n",
    "    print('the average loss after completion of %d epochs is %g'%((i+1),(avg_loss/len(X_sample))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(model, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './data/model_last.pth')\n",
    "# torch.save(model, './data/model_last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_params(model, './data/params.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "   0.1471  0.0785  0.0784  0.3393  0.5524  0.6816 -0.4937 -0.4309  0.1679\n",
       "\n",
       "Columns 9 to 17 \n",
       "   0.2782  0.3115 -0.1261 -0.0107 -1.1822 -0.2314  1.8432 -0.6515 -1.2931\n",
       "\n",
       "Columns 18 to 26 \n",
       "   0.2327 -0.0494  2.2513 -0.6735 -1.1014 -1.9060  0.8493 -1.3762  2.7118\n",
       "\n",
       "Columns 27 to 35 \n",
       "  -0.7583  0.9625  2.0209  0.0363 -0.3103 -0.1589 -0.7603 -0.0682  0.1950\n",
       "\n",
       "Columns 36 to 44 \n",
       "   0.1932 -2.1187 -0.7403  0.0552 -1.2713 -0.5040  0.0629 -1.9660  0.3906\n",
       "\n",
       "Columns 45 to 53 \n",
       "  -1.0398 -0.3503  0.0669  1.0941 -1.4916 -1.3252  0.4721  0.0430 -1.8012\n",
       "\n",
       "Columns 54 to 62 \n",
       "   0.6825 -0.1782  0.3976  0.4856  0.5159 -0.1171  0.6513  1.1366  0.3853\n",
       "\n",
       "Columns 63 to 71 \n",
       "   0.4935 -0.3607  0.9227 -0.8155  1.4683  0.9312  0.6763 -0.7830 -0.4143\n",
       "\n",
       "Columns 72 to 79 \n",
       "  -0.5284  0.5089  0.5376  0.3342  0.8130  0.3752  0.0829  0.8094\n",
       "[torch.cuda.FloatTensor of size 1x1x80 (GPU 0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(80,300,dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       " 5.6124e-03  3.8455e-02  2.5160e-02  ...   5.7284e-02 -4.3757e-02  1.4991e-02\n",
       " 4.4711e-02  3.2893e-02  4.0117e-02  ...  -6.1036e-03 -5.5793e-02  4.8791e-03\n",
       " 2.0104e-02  3.2321e-02 -5.7551e-02  ...   2.3971e-03 -2.3227e-02 -3.9308e-02\n",
       "                ...                   ⋱                   ...                \n",
       "-4.9176e-02  2.8848e-02 -5.1576e-02  ...  -3.8154e-02  1.0857e-02 -3.1291e-02\n",
       " 4.2728e-02 -5.5565e-02 -6.8024e-03  ...  -4.2155e-02  4.5272e-02  1.1601e-03\n",
       " 1.2782e-02 -3.6544e-02 -1.4150e-02  ...   1.3261e-03 -5.3807e-02 -5.2056e-02\n",
       "[torch.FloatTensor of size 900x300]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru.weight_hh_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
